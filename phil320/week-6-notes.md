# Philosophy of Mind Week 6
| Strong AI | Weak AI |
|-|-|
| Computers are minds | Computers simulate minds
| Computers can think and understand | Computers can't think or understand |
| Understanding is manipulating symbols | Understanding is deeper than that |
## The Chinese Room Argument
by John Searle in 1980

---
In this thought experiement, a native English speaker is in a room, isolated from other people. In this room, he has an instruction sheet that says if you get these Chinese characters, you should output these ones.

Eventually, he gets so good at outputting correct Chinese based on input that he doesn't need the cheat sheet. He doesn't understand Chinese. He is functioning like a computer, as a semantic engine (acting only on syntactic features). So, computers don't understand in the same way that this guy doesn't understand.

The argument intends to disprove **Strong AI**. Computers don't think, don't understand, and aren't minds.

### Counterarguments

The **Systems Reply** says that this guy doesn't understand Chinese, but the whole system (the guy and the instructions sheet) *do*. Searle's response to the Systems Reply is that it's absurd to say the guy doesn't understand Chinese, but the guy plus some bits of paper do. Understanding can only exist in the guy's head (mid response).

The **Robot Reply** (Causal Theory of Meaning) says we should put a computer in a robot that interacts with the world, and then the robot can have understanding. The causal theory of meaning says that we gain true understanding only through experiencing something. Searle says this robot still wouldn't understand, because the computer inside the robot is still only receiving symbolic information, whereas human minds are tied into our bodies.

The **Brain Simulation Reply** supposes that you could simulate the physical state of a brain with a computer, which is essentially just the physicalism argument.

The **Combination Reply** says what if we took a brain simulation, put it inside a robot, and put that into part of a system, surely that would have understanding. Searle reiterates the same counterpoint as he did for the Systems Reply: why should there be understanding in a system where the parts don't have understanding alone?

The **Other Minds Reply** assumes the problem of other minds (how do you know that anyone else has a mind?) to be true. If the guy in the Chinese room doesn't understand, then what happens if it were a native Chinese speaker in the room? They will still function just like a computer. If Searle is to deny understanding to the system, the computer, then you must deny understanding to other people as well. Searle says we are talking about whether there is understanding, not whether we can know it.

Finally, the **Many Mansions Reply** says there are ways other than programming that can yield understanding. There could be something other than symbol manipulation to understanding. Searle: Sure, but we are talking about Strong AI, so this counterargument is irrelevant.

> I think there is one important thing computers miss from human understanding. Human understanding is perceiving meaning. I know what that feels like, I know what that is. I don't think computers can do that, but how can I prove it? What is the conscious expereince of computers?