# Philosophy of Mind Week 6
| Strong AI | Weak AI |
|-|-|
| Computers are minds | Computers simulate minds
| Computers can think and understand | Computers can't think or understand |
| Understanding is manipulating symbols | Understanding is deeper than that |
## The Chinese Room Argument
by John Searle in 1980

---
In this thought experiement, a native English speaker is in a room, isolated from other people. In this room, he has an instruction sheet that says if you get these Chinese characters, you should output these ones.

Eventually, he gets so good at outputting correct Chinese based on input that he doesn't need the cheat sheet. He doesn't understand Chinese. He is functioning like a computer, as a semantic engine (acting only on syntactic features). So, computers don't understand in the same way that this guy doesn't understand.

The argument intends to disprove **Strong AI**. Computers don't think, don't understand, and aren't minds.

### Counterargument
What reason do we have to believe that this guy doesn't understand Chinese?

The **Systems Reply** says that this guy doesn't understand Chinese, but the whole system (the guy and the instructions sheet) *do*. Searle's response to the Systems Reply is that it's absurd to say the guy doesn't understand Chinese, but the guy plus some bits of paper do. Understanding can only exist in the guy's head (mid response).

The **Robot Reply** (Causal Theory of Meaning) says we should put a computer in a robot that interacts with the world, and then the robot can have understanding. The causal theory of meaning says that we gain true understanding only through experiencing something. Searle says this robot still wouldn't understand, because the computer inside the robot is still only receiving symbolic information, whereas human minds are tied into our bodies.

The **Brain Simulation Reply** supposes that you could simulate the physical state of a brain with a computer, which is essentially just the physicalism argument.